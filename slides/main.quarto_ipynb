{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Applied Large Language Models\"\n",
        "subtitle: ''\n",
        "author: Zach Dickson\n",
        "institute: Fellow in Quantitative Methodology <br>London School of Economics\n",
        "bibliography: references.bib\n",
        "format:\n",
        "  revealjs: \n",
        "    fontsize: 1.5em\n",
        "    logo: figures/LSE_logo.svg\n",
        "    embed-resources: true\n",
        "    slide-number: true\n",
        "    preview-links: auto\n",
        "    transition: convex\n",
        "    caption: true\n",
        "    tabularx: true\n",
        "    citation_package: biblatex\n",
        "    transition-speed: fast\n",
        "    theme: [simple, custom.scss]\n",
        "    footer: <a></a>\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Schedule {.scrollable .smaller}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img align=\"right\" src=\"figures/LLMS_wordcloud.jpg\" alt=\"Trulli\" style=\"width:45%\">\n",
        "<br>\n",
        "\n",
        "\n",
        "- A Brief Introduction to Large Language Models (LLMs) (50 minutes)\n",
        "  + Word embeddings vs. LLMs\n",
        "  + Pre-trained models (BERT, GPT)\n",
        "  + Applications in the social sciences \n",
        "  + Python basics \n",
        "\n",
        "**10 minute break**\n",
        "\n",
        "- Applied Example: Text Classification \n",
        "  - Fine-tune a transformer model to predict ideology \n",
        "  - Validation and verification\n",
        "\n",
        "**1 hour lunch**\n",
        "\n",
        "- Applied Example: Topic Modeling & Text Clustering \n",
        "  - Extract issue topics from parliamentary bills\n",
        "\n",
        "\n",
        "**10 minute break** \n",
        "\n",
        "- Everything else \n",
        "  + State-of-the-art applications\n",
        "  + Validating our models \n",
        "  + Limitations \n",
        "  + Future applications \n",
        "\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "# My Background & Research Interests\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# What are Large Language Models (LLMs)? \n",
        "\n",
        "- A language model is a machine learning model that intends to predict the next word in a sentence given the previous words. \n",
        "  - Example: Autocomplete on your phone \n",
        "\n",
        "- These models work by estimating the probability of a token (e.g. word), or a sequence of tokens, given the context of the sentence. \n",
        "  - Example: \"The cat is on the ___\" \n",
        "    + cup: 2.3%\n",
        "    + mat: 8.9%\n",
        "    + computer: 1.2%\n",
        "    + coffee: 0.9%\n",
        "  - The model predicts the next word is \"mat\" with the highest probability.\n",
        "  - A sequence of tokens could be a sentence, paragraph, or entire document.\n",
        "\n",
        "\n",
        "\n",
        "::: footer\n",
        "Introduction to Large Language Models\n",
        ":::\n",
        "\n",
        "\n",
        "# What are Large Language Models (LLMs)? \n",
        "\n",
        "- Modeling human language is very complex \n",
        "  - Syntax, semantics, pragmatics, etc. \n",
        "  - Context, ambiguity, and nuance \n",
        "  - Cultural and social norms\n",
        "\n",
        "- As models get larger, they can capture more of these complexities \n",
        "  - More parameters, more data, more context \n",
        "  - Better at predicting the next word in a sentence \n",
        "  - Better at understanding the meaning of words and sentences\n",
        "\n",
        "\n",
        "\n",
        "# Transformers \n",
        "\n",
        "\n",
        "- Transformers are a type of neural network architecture that has revolutionized natural language processing (NLP). \n",
        "  - Introduced by [Vaswani et al. (2017)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=attention+is+all+you+need&btnG=&oq=attention+i)\n",
        "  - The model is based on the concept of \"attention\"\n",
        "\n",
        "- Transformers consist of an encoder and a decoder \n",
        "  - The encoder processes the input sequence and produces a sequence of hidden states\n",
        "  - The decoder takes the hidden states produced by the encoder and generates the output sequence\n",
        "\n",
        "\n",
        "# Transformers Architecture\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img align=\"center\" src=\"figures/transformers_arc.png\" alt=\"Trulli\" style=\"width:95%\">\n",
        "</figure>\n",
        "\n",
        "\n",
        "# Attention Mechanism\n",
        "\n",
        "\n",
        "\n",
        "- Attention is a mechanism that allows the model to focus on different parts of the input sequence when making predictions. \n",
        "  - The model can learn which parts of the input are most important for making predictions. \n",
        "  - This allows the model to capture long-range dependencies in the data. \n",
        "  - The model can also learn to focus on different parts of the input depending on the context of the sentence.\n",
        "\n",
        "\n",
        "\n",
        "# Attention Mechanism\n",
        "\n",
        "\n",
        "![Attention Mechanism](figures/attention.png)\n",
        "\n",
        "\n",
        "\n",
        "# How do LLMs generate text? \n",
        "\n",
        "- LLMs generate text by sampling from the probability distribution over the vocabulary at each time step. \n",
        "  - The model predicts the next word in the sequence by sampling from the distribution over the vocabulary. \n",
        "  - The model can generate text one word at a time, or it can generate multiple words at once. \n",
        "  - The model can also generate text conditioned on a specific input, such as a prompt or a context. \n",
        "  - The model can generate text that is coherent and grammatically correct, but it can also generate text that is nonsensical or incoherent.\n",
        "\n",
        "- **Example:**\n",
        "  - \"My dog, Max, knows how to perform many traditional dog tricks. _______\"\n",
        "    - 2.3%: \"For example, he can sit, stay, and roll over.\"\n",
        "    - 2.1%: \"He can also fetch a ball, and he loves to play with his toys.\"\n",
        "\n",
        "\n",
        "\n",
        "# Pre-trained Models\n",
        "\n",
        "- Pre-trained models are large language models that have been trained on a large amount of text data \n",
        "  - Trained on a large corpus of text data, such as Wikipedia, news articles, and books \n",
        "  - Unsupervised learning, which means it does not require labeled data \n",
        "  - Trained for a long time, often several days or weeks\n",
        "\n",
        "- Pre-trained models can be fine-tuned on a specific task or dataset\n",
        "  - Fine-tuning involves updating the parameters of the pre-trained model on a smaller dataset that is specific to the task\n",
        "  - Fine-tuning allows the model to learn the specific patterns and relationships in the data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Pre-trained Models\n",
        "\n",
        "- Pre-trained models example: [BERT](https://huggingface.co/google-bert/bert-base-uncased) (Bi-directional Encoder Representations from Transformers)\n",
        "  - Introduced by [Devlin et al. (2018)](https://arxiv.org/abs/1810.04805)\n",
        "  - Pre-trained on a large corpus of text data, such as Wikipedia and news articles\n",
        "  - Fine-tuned on specific tasks, such as question answering, text classification, and named entity recognition\n",
        "\n",
        "\n",
        "\n",
        "# BERT \n"
      ],
      "id": "aa626c65"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker(\"Hello I'm a [MASK] model.\")"
      ],
      "id": "44a97af9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification, Categorization & Regression {.scrollable .smaller}\n",
        "\n",
        "<br>\n",
        "\n",
        " \n",
        "- Identify frames and narratives in political discourse: [Bailard et al. (2024)](https://doi.org/10.1017/S0003055423001478)\n",
        "  - Classify text by party, ideology or topic: [Lai et al. (2024)](https://doi.org/10.1017/pan.2023.42)\n",
        "  - Predicting Conflict Intensity: [HÃ¤ffner\n",
        " et al. (2023)](https://doi.org/10.1017/pan.2023.7) and [Wang (2023)](https://doi.org/10.1017/pan.2023.36)\n",
        "\n",
        "<figure>\n",
        "  <img src=\"figures/reddit.png\" alt=\"Trulli\" style=\"width:90%\" class=\"center\">\n",
        "  <figcaption>Source: Lai et al. (2024) in *Political Analysis*</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "::: footer\n",
        "Political Science Applications\n",
        ":::\n",
        "\n",
        "\n",
        "# LLMs in Survey Experiments and Polling {.scrollable .smaller}\n",
        "\n",
        "<br>\n",
        "\n",
        "- Political persuasion and micro-targeting: [Hackenburg & Margetts (2023)](https://files.osf.io/v1/resources/wnt8b/providers/osfstorage/64d8f1fa94a6be0e0212e744?action=download&direct&version=1) and [Simchon et al. (2024)](https://doi.org/10.1093/pnasnexus/pgae035)\n",
        "\n",
        "- Tailoring messages to specific audiences: [Mellon et al. (2024)](https://journals.sagepub.com/doi/10.1177/20531680241231468) and [Velez (n.d.)](figures/Latino_Issue_Publics.pdf)\n",
        "\n",
        "\n",
        "- Synthetic survey data generation: [Bisbee et al. (2023)](https://osf.io/preprints/socarxiv/5ecfa), [Sanders et al. (2023)](https://arxiv.org/abs/2307.04781) and [Simmons & Hare (2023)](https://arxiv.org/abs/2310.17888)\n",
        "  - A word of caution (see [Bisbee et al. (2023)](https://osf.io/preprints/socarxiv/5ecfa))\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img align=\"right\" src=\"figures/public_opinion.jpg\" alt=\"Trulli\" style=\"width:60%\">\n",
        "<br>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "# Some Applied Examples {.scrollable .smaller}\n",
        "\n",
        "\n",
        "\n",
        "1. Using the GPT-3.5 API\n",
        "2. Classification Validation\n",
        "3. Building your own classifier\n",
        "    a. validation and verification\n",
        "\n",
        "<br>\n",
        "\n",
        "[**Notebook**](https://colab.research.google.com/drive/1No_V3BzhWim9Zp1xfl6bfiiIYSoPwz9y?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img align=\"right\" src=\"figures/Hugging-Face2.png\" alt=\"Trulli\" style=\"width:60%\">\n",
        "<br>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img align=\"right\" src=\"figures/colab.png\" alt=\"Trulli\" style=\"width:60%\">\n",
        "<br>\n",
        "</figure>"
      ],
      "id": "22eb5bc6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/zach/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}